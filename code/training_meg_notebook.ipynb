{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ray import cloudpickle as cloudpickle\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'rest': 0, 'motor': 1, 'story': 2, 'memory': 3}\n",
    "downsample_to = int(35595 * 0.75)\n",
    "batch_size = 4\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(file_name_with_dir) :\n",
    "    filename_without_dir = file_name_with_dir.split('/')[-1]\n",
    "    temp = filename_without_dir.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name\n",
    "\n",
    "def get_dataset_from_dir(directory_name, steps_after_downsampling=1000, scaling='standard'): # NB: steps_after_downsampling can be seen as a hyperparameter\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    for filename in os.listdir(directory_name):\n",
    "        filename_path = directory_name + '/' + filename\n",
    "        with h5py.File(filename_path, 'r') as f:\n",
    "            dataset_name = get_dataset_name(filename_path)\n",
    "            data = f.get(dataset_name)\n",
    "            data = np.array(data)\n",
    "            if 'rest' in filename:\n",
    "                labels_list.append('rest')\n",
    "            elif 'motor' in filename:\n",
    "                labels_list.append('motor')\n",
    "            elif 'story' in filename:\n",
    "                labels_list.append('story')\n",
    "            elif 'memory' in filename:\n",
    "                labels_list.append('memory')\n",
    "            else:\n",
    "                raise ValueError(f'Inappropriate filename: {directory_name}/{filename}')\n",
    "\n",
    "        # scaling\n",
    "        if scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "            data_scaled = scaler.fit_transform(data) # NB each h5 file is scaled seperately\n",
    "        elif scaling =='minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "            data_scaled = scaler.fit_transform(data)\n",
    "        else:\n",
    "            data_scaled = data\n",
    "        # downsampling\n",
    "        if steps_after_downsampling is not None:\n",
    "            data_downsampled = resample(data_scaled, num=steps_after_downsampling, axis=1)\n",
    "        else:\n",
    "            data_downsampled = data_scaled\n",
    "        data_transposed = np.transpose(data_downsampled) # rows: timesteps, columns: sensors\n",
    "        data_list.append(data_transposed)\n",
    "    return data_list, labels_list\n",
    "\n",
    "# if you want to use the cross datasets, substitute the directories with \"Data/Cross/train/\" and \"Data/Cross/test{1, 2, 3}/\"\n",
    "# does assume that you have a Data directory in the same directory as this notebook\n",
    "cross_train_data_list, cross_train_labels_list = get_dataset_from_dir('Data/Cross/train/', steps_after_downsampling=downsample_to, scaling='standard')\n",
    "cross_test1_data_list, cross_test1_labels_list = get_dataset_from_dir('Data/Cross/test1/', steps_after_downsampling=downsample_to, scaling='standard')\n",
    "cross_test2_data_list, cross_test2_labels_list = get_dataset_from_dir('Data/Cross/test2/', steps_after_downsampling=downsample_to, scaling='standard')\n",
    "cross_test3_data_list, cross_test3_labels_list = get_dataset_from_dir('Data/Cross/test3/', steps_after_downsampling=downsample_to, scaling='standard')\n",
    "\n",
    "# sanity checks\n",
    "print(\"Are train and test the same shape?\", cross_train_data_list[0].shape == cross_test1_data_list[0].shape)\n",
    "print(\"Train shape:\", cross_train_data_list[0].shape)\n",
    "\n",
    "#print(cross_train_data_list[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the training and testing data arrays\n",
    "training_data_array = np.stack(cross_train_data_list)  # Stack training data arrays\n",
    "testing1_data_array = np.stack(cross_test1_data_list)   # Stack testing data arrays \n",
    "testing2_data_array = np.stack(cross_test2_data_list)   # Stack testing data arrays \n",
    "testing3_data_array = np.stack(cross_test3_data_list)   # Stack testing data arrays \n",
    "\n",
    "\n",
    "# Map labels to numerical values using label_mapping dictionary\n",
    "training_labels_array = np.array([label_mapping[label] for label in cross_train_labels_list])  # Convert training labels to numerical values\n",
    "testing1_labels_array = np.array([label_mapping[label] for label in cross_test1_labels_list])    # Convert testing labels to numerical values\n",
    "testing2_labels_array = np.array([label_mapping[label] for label in cross_test2_labels_list])    # Convert testing labels to numerical values\n",
    "testing3_labels_array = np.array([label_mapping[label] for label in cross_test3_labels_list])    # Convert testing labels to numerical values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert data arrays to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(training_data_array)  # Convert training data to PyTorch tensor\n",
    "y_train_tensor = torch.LongTensor(training_labels_array)  # Convert training labels to PyTorch tensor\n",
    "X_test1_tensor = torch.FloatTensor(testing1_data_array)    # Convert testing data to PyTorch tensor\n",
    "y_test1_tensor = torch.LongTensor(testing1_labels_array)    # Convert testing labels to PyTorch tensor\n",
    "X_test2_tensor = torch.FloatTensor(testing2_data_array)    # Convert testing data to PyTorch tensor\n",
    "y_test2_tensor = torch.LongTensor(testing2_labels_array)    # Convert testing labels to PyTorch tensor\n",
    "X_test3_tensor = torch.FloatTensor(testing3_data_array)    # Convert testing data to PyTorch tensor\n",
    "y_test3_tensor = torch.LongTensor(testing3_labels_array)    # Convert testing labels to PyTorch tensor\n",
    "\n",
    "# Create PyTorch datasets using tensors\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)  # Create dataset for training\n",
    "test1_dataset = TensorDataset(X_test1_tensor, y_test1_tensor)     # Create dataset for testing\n",
    "test2_dataset = TensorDataset(X_test2_tensor, y_test2_tensor)     # Create dataset for testing\n",
    "test3_dataset = TensorDataset(X_test3_tensor, y_test3_tensor)     # Create dataset for testing\n",
    "\n",
    "\n",
    "test_abs = int(len(train_dataset) * 0.8)\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    train_dataset, [test_abs, len(train_dataset) - test_abs]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)   # DataLoader for validation set with shuffling\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)   # DataLoader for validation set with shuffling\n",
    "test1_loader = DataLoader(test1_dataset, batch_size=batch_size, shuffle=False)   # DataLoader for testing set without shuffling\n",
    "test2_loader = DataLoader(test2_dataset, batch_size=batch_size, shuffle=False)   # DataLoader for testing set without shuffling\n",
    "test3_loader = DataLoader(test3_dataset, batch_size=batch_size, shuffle=False)   # DataLoader for testing set without shuffling\n",
    "\n",
    "\n",
    "for X, y in test1_loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "# Printing shape and data type of a batch from test_loader (for debugging or inspection purposes)\n",
    "for batch_idx, (data,  targets) in enumerate(train_loader):\n",
    "    print(f\"Batch Index: {batch_idx}\")\n",
    "    print(f\"Data shape: {data.shape}\")  # Shape of the input data tensor in the batch\n",
    "    print(f\"Targets shape: {targets.shape}\")  # Shape of the target labels tensor in the batch\n",
    "    break  # Break after printing the shape of the first batch for demonstration purposes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEG_LSTM(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = torch.nn.LSTM(248, self.hidden_size, self.num_layers, batch_first=True) # input size is 248, not configurable\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
    "        self.fc = torch.nn.Linear(hidden_size, 4) # output size is 4, not configurable\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))  # Passing input through LSTM\n",
    "        out = self.dropout(out) # Passing through dropout\n",
    "        # Get output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "class MEG_LSTM_Bidirectional(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM_Bidirectional, self).__init__()\n",
    "        self.input_size = 248 # input size is data from 248 sensors, not configurable\n",
    "        self.output_size = 4 # output size is 4 classes, not configurable\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = torch.nn.LSTM(248, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True) # input size is 248, not configurable\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
    "        self.fc = torch.nn.Linear(hidden_size * 2, 4) # output size is 4, not configurable\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # dropout\n",
    "        out = self.dropout(out)\n",
    "        # output layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class MEG_LSTM_Dense(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM_Dense, self).__init__()\n",
    "        self.input_size = 248\n",
    "        self.output_size = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = torch.nn.LSTM(248, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        # Additional dense layer\n",
    "        self.dense = torch.nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        # ReLU activation\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # Output layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * 2, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x)\n",
    "        # Dense layer\n",
    "        out = self.dense(out)\n",
    "        # Apply ReLU activation\n",
    "        out = self.relu(out)\n",
    "        # Output layer forward pass\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class MEG_LSTM_Expanded(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM_Expanded, self).__init__()\n",
    "        self.input_size = 248\n",
    "        self.output_size = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = torch.nn.LSTM(248, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        # Batch Normalization\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.hidden_size * 2)\n",
    "        # Dense layer\n",
    "        self.dense = torch.nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        # ReLU activation\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # Output layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * 2, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        out, _ = self.lstm(x)\n",
    "        # batch normalization\n",
    "        out = self.batch_norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # dense layer\n",
    "        out = self.dense(out)\n",
    "        # Apply ReLU activation\n",
    "        out = self.relu(out)\n",
    "        # Output layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class MEG_LSTM_Complex(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM_Complex, self).__init__()\n",
    "        self.input_size = 248\n",
    "        self.output_size = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm1 = torch.nn.LSTM(248, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        # tanh activation\n",
    "        self.tanh1 = torch.nn.Tanh()\n",
    "        # Batch normalization\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(self.hidden_size * 2)\n",
    "        # lstm layer 2\n",
    "        self.lstm2 = torch.nn.LSTM(self.hidden_size * 2, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        # tanh activation 2\n",
    "        self.tanh2 = torch.nn.Tanh()\n",
    "        # batch normalization 2\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(self.hidden_size * 2)\n",
    "        # dense layer\n",
    "        self.dense = torch.nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        # ELU activation\n",
    "        self.elu = torch.nn.ELU()\n",
    "        # Output layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * 2, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # lstm layer 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        # tanh layer 1\n",
    "        out = self.tanh1(out)\n",
    "        # batch normalization 1\n",
    "        out = self.batch_norm1(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # lstm layer 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        # tanh layer 2\n",
    "        out = self.tanh2(out)\n",
    "        # batch normalization 2\n",
    "        out = self.batch_norm2(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # dense layer\n",
    "        out = self.dense(out[:, -1, :])\n",
    "        # elu activation\n",
    "        out = self.elu(out)\n",
    "        # output layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class MEG_LSTM_Layered(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM_Layered, self).__init__()\n",
    "        self.input_size = 248\n",
    "        self.output_size = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # lstm layer 1\n",
    "        self.lstm1 = torch.nn.LSTM(248, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        # dropout layer 1\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout_prob)\n",
    "        # lstm layer 2\n",
    "        self.lstm2 = torch.nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        # dropout layer 2\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout_prob)\n",
    "        # lstm layer 3\n",
    "        self.lstm3 = torch.nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        # dropout layer 3\n",
    "        self.dropout3 = torch.nn.Dropout(p=dropout_prob)\n",
    "        self.fc = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout3(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping mechanism to speed up training\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=1):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.min_val_loss = float('inf')\n",
    "    \n",
    "    def stop_training(self, val_loss) -> bool:\n",
    "        \"\"\"Assess whether early stopping is necessary through validation loss.\"\"\"\n",
    "        if val_loss < self.min_val_loss:\n",
    "            self.min_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif val_loss > (self.min_val_loss + self.delta):\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, loss_function, num_epochs=10):\n",
    "    # initialize early stopping\n",
    "    stopper = EarlyStopping(patience=3, delta=0)\n",
    "    # begin training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = model(data)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}] Batch [{batch_idx + 1}/{len(train_loader)}] Loss: {running_loss / 100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for _, data in enumerate(val_loader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(\"cpu\"), labels.to(\"cpu\")\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = loss_function(outputs, labels)\n",
    "                print(f'val_loss: {loss}')\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "    \n",
    "        # check whether early stopping is necessary\n",
    "        if stopper.stop_training(val_loss):\n",
    "            print(f'stopped early at epoch {epoch}')\n",
    "            break\n",
    "        \n",
    "\n",
    "# Define the testing function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LSTM\n",
    "# Model instantiation\n",
    "hidden_size = 64  # Number of features in the hidden state of the GRU\n",
    "num_layers = 2  # Number of GRU layers\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model = MEG_LSTM(hidden_size, num_layers, dropout_prob)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10 # Number of epochs\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_function, num_epochs)\n",
    "\n",
    "# Testing the model\n",
    "ac1 = test_model(model, test1_loader)\n",
    "ac2 = test_model(model, test2_loader)\n",
    "ac3 = test_model(model, test3_loader)\n",
    "\n",
    "print(\"total accuracy of all test sets: \", float((ac1 + ac2 + ac3) / 3))\n",
    "\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'Models')\n",
    "with open(os.path.join(directory, model._get_name()), 'wb') as savefile:\n",
    "    pickle.dump('lol', savefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM\n",
    "# Model instantiation\n",
    "hidden_size = 64  # Number of features in the hidden state of the GRU\n",
    "num_layers = 2  # Number of GRU layers\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model = MEG_LSTM_Bidirectional(hidden_size, num_layers, dropout_prob)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10 # Number of epochs\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_function, num_epochs)\n",
    "\n",
    "# Testing the model\n",
    "ac1 = test_model(model, test1_loader)\n",
    "ac2 = test_model(model, test2_loader)\n",
    "ac3 = test_model(model, test3_loader)\n",
    "\n",
    "print(\"total accuracy of all test sets: \", float((ac1 + ac2 + ac3) / 3))\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'Models')\n",
    "with open(os.path.join(directory, model._get_name()), 'wb') as savefile:\n",
    "    pickle.dump(model, savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense + LSTM\n",
    "# Model instantiation\n",
    "hidden_size = 64  # Number of features in the hidden state of the GRU\n",
    "num_layers = 2  # Number of GRU layers\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model = MEG_LSTM_Dense(hidden_size, num_layers, dropout_prob)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10 # Number of epochs\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_function, num_epochs)\n",
    "\n",
    "# Testing the model\n",
    "ac1 = test_model(model, test1_loader)\n",
    "ac2 = test_model(model, test2_loader)\n",
    "ac3 = test_model(model, test3_loader)\n",
    "\n",
    "print(\"total accuracy of all test sets: \", float((ac1 + ac2 + ac3) / 3))\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'Models')\n",
    "with open(os.path.join(directory, model._get_name()), 'wb') as savefile:\n",
    "    pickle.dump(model, savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization + dense LSTM\n",
    "# Model instantiation\n",
    "hidden_size = 64  # Number of features in the hidden state of the GRU\n",
    "num_layers = 2  # Number of GRU layers\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model = MEG_LSTM_Expanded(hidden_size, num_layers, dropout_prob)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10 # Number of epochs\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_function, num_epochs)\n",
    "\n",
    "# Testing the model\n",
    "ac1 = test_model(model, test1_loader)\n",
    "ac2 = test_model(model, test2_loader)\n",
    "ac3 = test_model(model, test3_loader)\n",
    "\n",
    "print(\"total accuracy of all test sets: \", float((ac1 + ac2 + ac3) / 3))\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'Models')\n",
    "with open(os.path.join(directory, model._get_name()), 'wb') as savefile:\n",
    "    pickle.dump(model, savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comples LSTM\n",
    "# Model instantiation\n",
    "hidden_size = 64  # Number of features in the hidden state of the GRU\n",
    "num_layers = 2  # Number of GRU layers\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model = MEG_LSTM_Complex(hidden_size, num_layers, dropout_prob)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10 # Number of epochs\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_function, num_epochs)\n",
    "\n",
    "# Testing the model\n",
    "ac1 = test_model(model, test1_loader)\n",
    "ac2 = test_model(model, test2_loader)\n",
    "ac3 = test_model(model, test3_loader)\n",
    "\n",
    "print(\"total accuracy of all test sets: \", float((ac1 + ac2 + ac3) / 3))\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'Models')\n",
    "with open(os.path.join(directory, model._get_name()), 'wb') as savefile:\n",
    "    pickle.dump(model, savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layered LSTM\n",
    "# Model instantiation\n",
    "hidden_size = 64  # Number of features in the hidden state of the GRU\n",
    "num_layers = 2  # Number of GRU layers\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model = MEG_LSTM_Layered(hidden_size, num_layers, dropout_prob)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10 # Number of epochs\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_function, num_epochs)\n",
    "\n",
    "# Testing the model\n",
    "ac1 = test_model(model, test1_loader)\n",
    "ac2 = test_model(model, test2_loader)\n",
    "ac3 = test_model(model, test3_loader)\n",
    "\n",
    "print(\"total accuracy of all test sets: \", float((ac1 + ac2 + ac3) / 3))\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'Models')\n",
    "with open(os.path.join(directory, model._get_name()), 'wb') as savefile:\n",
    "    pickle.dump(model, savefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
