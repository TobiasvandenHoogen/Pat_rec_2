{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.signal import resample\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "import torchmetrics\n",
    "from torch.utils.data import random_split\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray import cloudpickle as cloudpickle\n",
    "import pickle\n",
    "from ray.train import Checkpoint\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'rest': 0, 'motor': 1, 'story': 2, 'memory': 3}\n",
    "downsample_rate = 35595 * (1 - 0.1)\n",
    "batch_size = 4\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(file_name_with_dir) :\n",
    "    filename_without_dir = file_name_with_dir.split('/')[-1]\n",
    "    temp = filename_without_dir.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name\n",
    "\n",
    "def get_dataset_from_dir(dir_name, label_mapper, downsample_rate=0.0, scaling='standard', use_skorch=False):\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    for filename in os.listdir(dir_name):\n",
    "        filename_path = dir_name + '/' + filename\n",
    "        with h5py.File(filename_path, 'r') as f:\n",
    "            dataset_name = get_dataset_name(filename_path)\n",
    "            data = f.get(dataset_name)\n",
    "            data = np.array(data)\n",
    "            if 'rest' in filename:\n",
    "                labels_list.append('rest')\n",
    "            elif 'motor' in filename:\n",
    "                labels_list.append('motor')\n",
    "            elif 'story' in filename:\n",
    "                labels_list.append('story')\n",
    "            elif 'memory' in filename:\n",
    "                labels_list.append('memory')\n",
    "            else:\n",
    "                raise ValueError(f'Inappropriate filename: {dir_name}/{filename}')\n",
    "\n",
    "        # scaling\n",
    "        if scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "            data_scaled = scaler.fit_transform(data) # NB each h5 file is scaled seperately\n",
    "        elif scaling =='minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "            data_scaled = scaler.fit_transform(data)\n",
    "        else:\n",
    "            data_scaled = data\n",
    "        # downsampling\n",
    "        if downsample_rate > 0:\n",
    "            steps_after_downsampling = int(35595 * (1 - downsample_rate))\n",
    "            data_downsampled = resample(data_scaled, num=steps_after_downsampling, axis=1)\n",
    "        else:\n",
    "            data_downsampled = data_scaled\n",
    "        data_transposed = np.transpose(data_downsampled) # rows: timesteps, columns: sensors\n",
    "        data_list.append(data_transposed)\n",
    "    features = np.stack(data_list)\n",
    "    labels = np.array([label_mapper[label] for label in labels_list])\n",
    "    if use_skorch:\n",
    "        return features, labels\n",
    "    \n",
    "    # Convert data arrays to PyTorch tensors\n",
    "    X_tensor = torch.FloatTensor(features) # Convert training data to PyTorch tensor\n",
    "    y_tensor = torch.LongTensor(labels) # Convert testing labels to PyTorch tensor\n",
    "\n",
    "    # Create PyTorch datasets using tensors\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)  # Create dataset for training\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEG_LSTM_Bidirectional(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM_Bidirectional, self).__init__()\n",
    "        self.input_size = 248 # input size is data from 248 sensors, not configurable\n",
    "        self.output_size = 4 # output size is 4 classes, not configurable\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = torch.nn.LSTM(248, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True) # input size is 248, not configurable\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
    "        self.fc = torch.nn.Linear(hidden_size * 2, 4) # output size is 4, not configurable\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))  # Passing input through LSTM\n",
    "        out = self.dropout(out)\n",
    "        # Get output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class MEG_LSTM(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = torch.nn.LSTM(248, self.hidden_size, self.num_layers, bidirectional=True, batch_first=True) # input size is 248, not configurable\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_prob)\n",
    "        self.fc = torch.nn.Linear(hidden_size, 4) # output size is 4, not configurable\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))  # Passing input through LSTM\n",
    "        \n",
    "        # Get output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "class MEG_LSTM_Complex(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=2, dropout_prob=0.1):\n",
    "        super(MEG_LSTM_Complex, self).__init__()\n",
    "        self.input_size = 248\n",
    "        self.output_size = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm1 = torch.nn.LSTM(self.input_size, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        self.tanh1 = torch.nn.Tanh()\n",
    "        # Batch Normalization\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(self.hidden_size * 2)\n",
    "        self.lstm2 = torch.nn.LSTM(self.hidden_size * 2, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        self.tanh2 = torch.nn.Tanh()\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(self.hidden_size * 2)\n",
    "        self.dense = torch.nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        # ReLU activation\n",
    "        self.elu = torch.nn.ELU()\n",
    "        # Output layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * 2, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.tanh1(out)\n",
    "        out = self.batch_norm1(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.tanh2(out)\n",
    "        out = self.batch_norm2(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        out = self.dense(out[:, -1, :])\n",
    "        out = self.elu(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MEG_GRU_Complex(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, dropout_prob):\n",
    "        super(MEG_GRU_Complex, self).__init__()\n",
    "        self.input_size = 248\n",
    "        self.output_size = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.gru1 = torch.nn.GRU(self.input_size, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        self.tanh1 = torch.nn.Tanh()\n",
    "        # Batch Normalization\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(self.hidden_size * 2)\n",
    "        self.gru2 = torch.nn.GRU(self.hidden_size * 2, self.hidden_size, self.num_layers, dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        self.tanh2 = torch.nn.Tanh()\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(self.hidden_size * 2)\n",
    "        self.dense = torch.nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        # ReLU activation\n",
    "        self.elu = torch.nn.ELU()\n",
    "        # Output layer\n",
    "        self.fc = torch.nn.Linear(hidden_size * 2, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru1(x)\n",
    "        out = self.tanh1(out)\n",
    "        out = self.batch_norm1(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        out, _ = self.gru2(out)\n",
    "        out = self.tanh2(out)\n",
    "        out = self.batch_norm2(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        out = self.dense(out[:, -1, :])\n",
    "        out = self.elu(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping mechanism to speed up training\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_val_loss = float('inf')\n",
    "    \n",
    "    def stop_training(self, val_loss) -> bool:\n",
    "        \"\"\"Assess whether early stopping is necessary through validation loss.\"\"\"\n",
    "        if val_loss < self.min_val_loss:\n",
    "            self.min_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif val_loss > self.min_val_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_meg(config, trainset, cwd):\n",
    "    net = MEG_GRU_Complex(hidden_size=config[\"hidden_size\"], num_layers=config[\"nr_layers\"], dropout_prob=config[\"dropout_rate\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    net.to(device)\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # initialize train- and testloader\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = DataLoader(train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8)\n",
    "    valloader = DataLoader(val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8)\n",
    "\n",
    "    # initialize early stopping\n",
    "    stopper = EarlyStopping(patience=3)\n",
    "\n",
    "    # start training loop\n",
    "    for epoch in range(0, config[\"max_epochs\"]):\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # data is a list with shape [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward step\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            # backward step\n",
    "            loss.backward()\n",
    "            \n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 5 == 4: # return statistics every 5 minibatches\n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / epoch_steps}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        # Store a checkpoint as dict:\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(cwd, 'Checkpoints/checkpoint_data.pkl'), 'wb') as fp:\n",
    "            cloudpickle.dump(checkpoint_data, fp)\n",
    "\n",
    "        checkpoint = Checkpoint.from_directory(os.path.join(cwd, 'Checkpoints'))\n",
    "        train.report({\"loss\": val_loss / val_steps, \"accuracy\": correct / total}, checkpoint=checkpoint)\n",
    "\n",
    "        # check whether early stopping is necessary\n",
    "        if stopper.stop_training(val_loss):\n",
    "            print(f'stopped early at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "def eval_model(net, testset, device=\"cpu\"):\n",
    "\n",
    "    # initialize test loader\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "    result_list = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # calculate statistics\n",
    "    metric1 = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)\n",
    "    metric2 = torchmetrics.Precision(task=\"multiclass\", average=\"macro\", num_classes=4)\n",
    "    metric3 = torchmetrics.Recall(task=\"multiclass\", average=\"macro\", num_classes=4)\n",
    "    metric4 = torchmetrics.F1Score(task=\"multiclass\", average=\"macro\", num_classes=4)\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for data in testloader:\n",
    "            features, labels = data\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = net(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            acc = metric1(predicted, labels)\n",
    "            pre = metric2(predicted, labels)\n",
    "            rec = metric3(predicted, labels)\n",
    "            f1 = metric4(predicted, labels)\n",
    "            for pred, lab in zip(predicted, labels):\n",
    "                if pred == lab:\n",
    "                    result_list.append(1)\n",
    "                else:\n",
    "                    result_list.append(0)\n",
    "            i += 1\n",
    "    acc = metric1.compute()\n",
    "    pre = metric2.compute()\n",
    "    rec = metric3.compute()\n",
    "    f1 = metric4.compute()\n",
    "    print(f\"Accuracy {i}: {acc}\")\n",
    "    print(f\"Precision {i}: {pre}\")\n",
    "    print(f\"Recall {i}: {rec}\")\n",
    "    print(f\"F1 {i}: {f1}\")\n",
    "    return[acc, pre, rec, f1, result_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_dir: str, downsample_to: float, num_samples: int):\n",
    "    print(f'Using downsampling rate: {downsample_to}.')\n",
    "    cwd = os.getcwd()\n",
    "    label_mapping = {'rest': 0, 'motor': 1, 'story': 2, 'memory': 3}\n",
    "    \n",
    "    train_set = get_dataset_from_dir(dir_name=train_dir, label_mapper=label_mapping, downsample_rate=downsample_to, scaling=\"standard\")\n",
    "\n",
    "    config = {\n",
    "            \"lr\": tune.choice([1e-4, 1e-3, 1e-2]), # 3\n",
    "            \"hidden_size\": tune.choice([32, 64, 128]), # 3\n",
    "            \"nr_layers\": tune.choice([2, 3, 4]), # 3\n",
    "            \"batch_size\": tune.choice([4, 8, 16]), # 3\n",
    "            \"max_epochs\": tune.choice([8]), # 1\n",
    "            \"dropout_rate\": tune.choice([0.1, 0.25, 0.5]) # 3\n",
    "    }\n",
    "\n",
    "    result = tune.run(\n",
    "        tune.with_parameters(train_meg, trainset=train_set, cwd=cwd),\n",
    "        resources_per_trial={\"cpu\": 11, \"gpu\": 0},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "    )\n",
    "\n",
    "    print('getting best trial')\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    if best_trial is None:\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Best val_loss: {best_trial.last_result['loss']}\")\n",
    "        print(f\"Best accuracy: {best_trial.last_result['accuracy']}\")\n",
    "        print(f\"Best config: {best_trial.config}\")\n",
    "\n",
    "        print('initializing best model so far')\n",
    "        best_trained_model = MEG_GRU_Complex(best_trial.config[\"hidden_size\"], best_trial.config[\"nr_layers\"], best_trial.config[\"dropout_rate\"])\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        best_trained_model.to(device)\n",
    "        \n",
    "        best_checkpoint = best_trial.checkpoint\n",
    "\n",
    "        # load checkpoint\n",
    "        if best_checkpoint is not None:\n",
    "            with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "                with open(os.path.join(checkpoint_dir, 'checkpoint_data.pkl'), 'rb') as fp:\n",
    "                    checkpoint = cloudpickle.load(fp)\n",
    "\n",
    "            best_trained_model.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "\n",
    "            test_set1 = get_dataset_from_dir(dir_name=\"Data/Cross/test1/\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "            test_set2 = get_dataset_from_dir(dir_name=\"Data/Cross/test2/\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "            test_set3 = get_dataset_from_dir(dir_name=\"Data/Cross/test3/\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "\n",
    "            test_acc1 = eval_model(best_trained_model, test_set1)\n",
    "            test_acc2 = eval_model(best_trained_model, test_set2)\n",
    "            test_acc3 = eval_model(best_trained_model, test_set3)\n",
    "            print(\"Best trial test set accuracy: {}\".format(test_acc1 + test_acc2 + test_acc3))\n",
    "            with open(os.path.join(cwd, f'Models/{best_trained_model._get_name()}'), 'wb') as model_file:\n",
    "                pickle.dump(best_trained_model, model_file)\n",
    "\n",
    "main(train_dir='Data/Cross/train/', downsample_to=0.9, num_samples=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the hidden size an number of layers of the checkpoint\n",
    "best_trained_model = MEG_LSTM_Complex(128, 3, 0.1)\n",
    "\n",
    "# Move the best checkpoint into the checkpoint folder\n",
    "with open(os.path.join(\"Checkpoints\", 'checkpoint_lstm_intra.pkl'), 'rb') as fp:\n",
    "    checkpoint = cloudpickle.load(fp)\n",
    "\n",
    "best_trained_model.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "\n",
    "test_set1 = get_dataset_from_dir(dir_name=\"Data/intra/test\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "# test_set2 = get_dataset_from_dir(dir_name=\"Data/Cross/test2/\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "# test_set3 = get_dataset_from_dir(dir_name=\"Data/Cross/test3/\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "\n",
    "a1 = eval_model(best_trained_model, test_set1)\n",
    "# a2 = eval_model(best_trained_model, test_set2)\n",
    "# a3 = eval_model(best_trained_model, test_set3)\n",
    "# print(\"Best trial test set accuracy: {}\".format((a1[0] + a2[0] + a3[0]) / 3))\n",
    "# print(\"Best trial test set precision: {}\".format((a1[1] + a2[1] + a3[1]) / 3))\n",
    "# print(\"Best trial test set accuracy: {}\".format((a1[2] + a2[2] + a3[2]) / 3))\n",
    "# print(\"Best trial test set accuracy: {}\".format((a1[3] + a2[3] + a3[3]) / 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the hidden size an number of layers of the checkpoint\n",
    "model1 = MEG_LSTM_Complex(128, 3, 0.1)\n",
    "model2 = MEG_GRU_Complex(128, 3, 0.1)\n",
    "\n",
    "\n",
    "# Move the best checkpoint into the checkpoint folder\n",
    "with open(os.path.join(\"Checkpoints\", 'checkpoint_lstm_intra.pkl'), 'rb') as fp:\n",
    "    checkpoint1 = cloudpickle.load(fp)\n",
    "\n",
    "with open(os.path.join(\"Checkpoints\", 'checkpoint_gru_intra.pkl'), 'rb') as fp:\n",
    "    checkpoint2 = cloudpickle.load(fp)\n",
    "\n",
    "model1.load_state_dict(checkpoint1[\"net_state_dict\"])\n",
    "model2.load_state_dict(checkpoint2[\"net_state_dict\"])\n",
    "\n",
    "\n",
    "test_set1 = get_dataset_from_dir(dir_name=\"Data/intra/test\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "# test_set2 = get_dataset_from_dir(dir_name=\"Data/Cross/test2/\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "# test_set3 = get_dataset_from_dir(dir_name=\"Data/Cross/test3/\", label_mapper=label_mapping, downsample_rate=0.9, scaling=\"standard\")\n",
    "\n",
    "e1 = eval_model(model1, test_set1)\n",
    "e2 = eval_model(model2, test_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare arrays to use for statistical tests\n",
    "lstm_correct = e1[4]\n",
    "gru_correct = e2[4]\n",
    "\n",
    "def perform_mcnemar_test(model1_prediction_vals, model2_prediction_vals):\n",
    "    # List which contain data entries in which both the models predicted correctly \n",
    "    both = sum([logit_val and svm_val for logit_val, svm_val in zip(lstm_correct,gru_correct)])\n",
    "    # List which contain data entries in which only model 1 predicted correctly \n",
    "    model1 = sum([logit_val and not svm_val for logit_val, svm_val in zip(lstm_correct, gru_correct)])\n",
    "    # List which contain data entries in which only model 2 predicted correctly \n",
    "    model2 = sum([not logit_val and svm_val for logit_val, svm_val in zip(lstm_correct, gru_correct)])\n",
    "    # List which contain data entries in which both the models predicted incorrectly \n",
    "    neither = sum([not logit_val and not svm_val for logit_val, svm_val in zip(lstm_correct, gru_correct)])\n",
    "\n",
    "    # Merge these lists into a matrix for the Mcnemar test \n",
    "    contingencies = [[both, model1],\n",
    "                     [model2, neither]]\n",
    "\n",
    "    # Perform Mcnemar test with the given contingencies \n",
    "    mcnemar_statistic = ((model1 - model2)**2)/(model1 + model2)\n",
    "    mcnemar_pvalue = stats.chi2.sf(mcnemar_statistic, 1)\n",
    "    print('McNemar\\'s test to verify significant difference between the models:\\n', 'Statistic: %.3f' % (mcnemar_statistic), 'p-value:', mcnemar_pvalue)\n",
    "\n",
    "perform_mcnemar_test(lstm_correct, gru_correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
